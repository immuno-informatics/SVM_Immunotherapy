{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import _config as cfg  # noqa: E402\n",
    "import DataSets_validation_CONCAT as ds  # noqa: E402\n",
    "import kaplanmeier as km  # noqa: E402\n",
    "import matplotlib.pyplot as plt  # noqa: E402\n",
    "import numpy as np  # noqa: E402\n",
    "import pandas as pd  # noqa: E402\n",
    "from imblearn.over_sampling import SMOTE  # noqa: E402\n",
    "from sklearnex import patch_sklearn  # noqa: E402\n",
    "\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn import svm  # noqa: E402\n",
    "from sklearn.metrics import (  # noqa: E402\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from tqdm.notebook import tqdm  # noqa: E402\n",
    "\n",
    "yeloh_seed = 2137\n",
    "\n",
    "random.seed(1024)\n",
    "np.random.seed(1024)\n",
    "\n",
    "results_dir = Path(\"Results\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "validation_dir = results_dir / \"validation\"\n",
    "validation_dir.mkdir(parents=True, exist_ok=True)\n",
    "validation_clinical_features_file = \"/data/teamgdansk/katy-variants/validation-datasets/KATY_Edin_cohort_clinical_features.csv\"\n",
    "\n",
    "\n",
    "def svm_model_file(model_name, cut_input_params=False):\n",
    "    model_name = model_name.replace(\" \", \"_\")\n",
    "    if cut_input_params:\n",
    "        name_suffix = \"-cut\"\n",
    "    else:\n",
    "        name_suffix = \"-full\"\n",
    "    return results_dir / f\"svm-model-{model_name}{name_suffix}.pkl\"\n",
    "\n",
    "\n",
    "# Matplotlib config\n",
    "plt.rcdefaults()\n",
    "\n",
    "main_font_size = 6\n",
    "label_font_size = 10\n",
    "\n",
    "axis_color = \"0.15\"\n",
    "\n",
    "plt.rcParams[\"text.color\"] = axis_color\n",
    "plt.rcParams[\"axes.labelcolor\"] = axis_color\n",
    "plt.rcParams[\"xtick.color\"] = axis_color\n",
    "plt.rcParams[\"ytick.color\"] = axis_color\n",
    "plt.rcParams[\"axes.edgecolor\"] = axis_color\n",
    "\n",
    "plt.rcParams[\"font.size\"] = main_font_size\n",
    "plt.rcParams[\"axes.labelsize\"] = main_font_size\n",
    "plt.rcParams[\"axes.titlesize\"] = main_font_size\n",
    "plt.rcParams[\"xtick.labelsize\"] = main_font_size\n",
    "plt.rcParams[\"ytick.labelsize\"] = main_font_size\n",
    "plt.rcParams[\"legend.fontsize\"] = main_font_size\n",
    "plt.rcParams[\"figure.labelsize\"] = main_font_size\n",
    "plt.rcParams[\"figure.titlesize\"] = main_font_size\n",
    "\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "\n",
    "plt.rcParams[\"savefig.transparent\"] = True\n",
    "\n",
    "plt.rcParams[\"pdf.fonttype\"] = 42\n",
    "plt.rcParams[\"ps.fonttype\"] = 42\n",
    "plt.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "\n",
    "\n",
    "def figsize_in_mm(width, height):\n",
    "    mm = 1 / 25.4  # millimeters in inches\n",
    "    return (width * mm, height * mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train_test(train_data, train_y, test_data, verbose=False, clf_kwargs={}):\n",
    "    sm = SMOTE(random_state=432)\n",
    "    train_data, train_y = sm.fit_resample(train_data, train_y)\n",
    "    clf = svm.SVC(probability=True, random_state=yeloh_seed, **clf_kwargs)\n",
    "    clf = clf.fit(train_data, train_y)\n",
    "\n",
    "    predictions = clf.predict(test_data)\n",
    "    probs = clf.predict_proba(test_data)\n",
    "    probSV = [i[1] for i in probs]\n",
    "    if verbose:\n",
    "        print(probSV)\n",
    "    new_pd = pd.DataFrame(probSV)\n",
    "    return clf, predictions, new_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "# Set `True` if you want to use only age, gender, and mutation data:\n",
    "cut_input_params = True\n",
    "\n",
    "# Figures config\n",
    "legend_font_size = 5\n",
    "text_font_size = 5\n",
    "\n",
    "fig_roc_size = figsize_in_mm(48, 37)\n",
    "\n",
    "fig_surv_size = figsize_in_mm(56, 36)\n",
    "\n",
    "fig_sc_size = figsize_in_mm(100, 40)\n",
    "fig_sc_bar_width = 0.2\n",
    "\n",
    "#\n",
    "\n",
    "if cut_input_params:\n",
    "    name_suffix = \"-cut\"\n",
    "else:\n",
    "    name_suffix = \"-full\"\n",
    "\n",
    "mut_vec_len_label = \"mut_vec_len\"\n",
    "clf_params_label = \"clf_params\"\n",
    "model_name_label = \"plot_label\"\n",
    "\n",
    "m_num = len(cfg.configurations)\n",
    "\n",
    "model_colors = {n[model_name_label]: f\"C{i}\" for i, n in enumerate(cfg.configurations)}\n",
    "model_colors_surv_lines = {}\n",
    "i = 0\n",
    "for n in cfg.configurations:\n",
    "    lines_c = [plt.color_sequences[\"tab20\"][i + 1], plt.color_sequences[\"tab20\"][i]]\n",
    "    model_colors_surv_lines[n[model_name_label]] = lines_c\n",
    "    i += 2\n",
    "\n",
    "fig_roc = plt.figure(figsize=fig_roc_size, layout=\"constrained\")\n",
    "ax_roc = fig_roc.add_subplot()\n",
    "\n",
    "test_scores_str = \"\"\n",
    "test_scores_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\"]\n",
    "test_scores = {}\n",
    "\n",
    "surv_ylabel = True\n",
    "\n",
    "for config in tqdm(cfg.configurations, leave=False):\n",
    "    dimension_of_embedding_vectors = config[mut_vec_len_label]\n",
    "    clf_params = config[clf_params_label]\n",
    "    model_name = config[model_name_label]\n",
    "\n",
    "    train_data, train_y, test_data, test_y, test_pfs, _, _, test_pfs_cnsr, _, _, _ = (\n",
    "        ds.transforming_Braun_dataset(\n",
    "            config,\n",
    "            dimension_of_embedding_vectors=dimension_of_embedding_vectors,\n",
    "            cut_input_params=cut_input_params,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model, svm_linear_preds, svm_prob = svm_train_test(\n",
    "        train_data, train_y, test_data, clf_kwargs=clf_params\n",
    "    )\n",
    "\n",
    "    probs = svm_prob.values.flatten()\n",
    "    fpr, tpr, _ = roc_curve(test_y, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    prec = precision_score(test_y, svm_linear_preds)\n",
    "    rec = recall_score(test_y, svm_linear_preds)\n",
    "    f1 = f1_score(test_y, svm_linear_preds)\n",
    "    acc = accuracy_score(test_y, svm_linear_preds)\n",
    "    rauc = roc_auc_score(test_y, probs)\n",
    "\n",
    "    ba = balanced_accuracy_score(test_y, svm_linear_preds)\n",
    "\n",
    "    test_scores_str += f\"{model_name}:\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[0]}: {acc:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[1]}: {prec:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[2]}: {rec:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[3]}: {f1:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[4]}: {rauc:.3f}\\n\"\n",
    "    test_scores_str += f\"    _MEAN_: {(acc + prec + rec + f1 + rauc) / 5:.3f}\\n\"\n",
    "    test_scores_str += f\"    BA: {ba:.3f}\\n\"\n",
    "\n",
    "    test_scores[model_name] = {}\n",
    "    test_scores[model_name][test_scores_names[0]] = acc\n",
    "    test_scores[model_name][test_scores_names[1]] = prec\n",
    "    test_scores[model_name][test_scores_names[2]] = rec\n",
    "    test_scores[model_name][test_scores_names[3]] = f1\n",
    "    test_scores[model_name][test_scores_names[4]] = rauc\n",
    "\n",
    "    _ = ax_roc.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        label=f\"{model_name} (AUC={roc_auc:.3f})\",\n",
    "        color=model_colors[model_name],\n",
    "        linewidth=1,\n",
    "    )\n",
    "\n",
    "    with open(svm_model_file(model_name, cut_input_params), \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # Response prediction\n",
    "    time_event = test_pfs.reset_index(drop=True)\n",
    "    censoring = test_pfs_cnsr.reset_index(drop=True)\n",
    "    # censoring = pd.Series([1 if p == -1 else 0 for p in svm_linear_preds])\n",
    "    labx = pd.Series(\n",
    "        [\"Responders\" if y == 1 else \"Non-responders\" for y in svm_linear_preds]\n",
    "    )\n",
    "    # labx = pd.Series([\"Responders\" if y == 1 else \"Non-responders\" for y in test_y])\n",
    "    surv_pred = km.fit(time_event, censoring, labx)\n",
    "    logrank_p = surv_pred[\"logrank_P\"]\n",
    "    fig_surv, ax_surv = km.plot(\n",
    "        surv_pred,\n",
    "        figsize=fig_surv_size,\n",
    "        fontsize=main_font_size,\n",
    "        cmap=model_colors_surv_lines[model_name],\n",
    "    )\n",
    "    # old_title = ax_surv.get_title()\n",
    "    # p_value_str = old_title.replace(\"Survival function\\n\", \"\")\n",
    "    ax_surv.set_title(None)\n",
    "    ax_surv.text(\n",
    "        0.85,\n",
    "        0.3,\n",
    "        f\"Logrank P-value:\\n{logrank_p:.3e}\",\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        transform=ax_surv.transAxes,\n",
    "        fontsize=text_font_size,\n",
    "    )\n",
    "    ax_surv.set_title(f\"{model_name}\")\n",
    "    # ax_surv.set_title(\n",
    "    #     old_title.replace(\"Survival function\", f\"{model_name}\"),\n",
    "    #     {\"fontsize\": label_font_size},\n",
    "    # )\n",
    "    ax_surv.tick_params(length=3.5)\n",
    "    ax_surv.set_xlabel(\"Time (months)\")\n",
    "    if surv_ylabel:\n",
    "        ax_surv.set_ylabel(\"Probability of response\")\n",
    "        surv_ylabel = False\n",
    "    ax_surv.legend(\n",
    "        handlelength=0.8,\n",
    "        handletextpad=0.5,\n",
    "        labelspacing=0.8,\n",
    "        fontsize=legend_font_size,\n",
    "        loc=\"upper right\",\n",
    "        # loc=\"upper center\",\n",
    "        # bbox_to_anchor=(0.5, -0.4),\n",
    "    )\n",
    "    for fmt in [\"pdf\", \"png\"]:\n",
    "        fig_surv.savefig(\n",
    "            results_dir.joinpath(\n",
    "                f\"response-prediction-{model_name.replace(' ', '_')}{name_suffix}.{fmt}\"\n",
    "            ),\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0.03,\n",
    "            dpi=600,\n",
    "        )\n",
    "\n",
    "# ROC\n",
    "ax_roc.set_xlabel(\"False Positive Rate\")\n",
    "ax_roc.set_ylabel(\"True Positive Rate\")\n",
    "# ax_roc.set_title(\"ROC Curves for different levels\", {\"fontsize\": label_font_size})\n",
    "ax_roc.legend(loc=\"lower right\", fontsize=legend_font_size)\n",
    "for fmt in [\"pdf\", \"png\"]:\n",
    "    fig_roc.savefig(results_dir.joinpath(f\"roc-curves{name_suffix}.{fmt}\"), dpi=600)\n",
    "\n",
    "# Print and save scores to a file\n",
    "print(test_scores_str)\n",
    "with open(results_dir.joinpath(f\"scores{name_suffix}.txt\"), \"w\") as f:\n",
    "    f.write(test_scores_str)\n",
    "\n",
    "# Scores figure\n",
    "m_names = list(test_scores.keys())\n",
    "s_num = len(test_scores_names)\n",
    "x = np.arange(s_num)\n",
    "b_pos = np.arange(\n",
    "    fig_sc_bar_width / 2 * (1 - m_num),\n",
    "    fig_sc_bar_width / 2 * (m_num - 1),\n",
    "    fig_sc_bar_width,\n",
    ")\n",
    "fig_sc = plt.figure(figsize=fig_sc_size, layout=\"constrained\")\n",
    "ax_sc = fig_sc.add_subplot()\n",
    "for n, p in zip(m_names, b_pos):\n",
    "    _ = ax_sc.bar(\n",
    "        x + p,\n",
    "        list(test_scores[n].values()),\n",
    "        width=fig_sc_bar_width,\n",
    "        label=n,\n",
    "        color=model_colors[n],\n",
    "        zorder=10,\n",
    "    )\n",
    "ax_sc.set_xticks(x, test_scores_names)\n",
    "ax_sc.grid(axis=\"y\")\n",
    "_ = ax_sc.legend(\n",
    "    fontsize=legend_font_size, ncols=m_num, loc=\"lower right\", bbox_to_anchor=(1, 1)\n",
    ")\n",
    "for fmt in [\"pdf\", \"png\"]:\n",
    "    fig_sc.savefig(results_dir.joinpath(f\"scores{name_suffix}.{fmt}\"), dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "# Set `True` if you want to use only age, gender, and mutation data:\n",
    "cut_input_params = True\n",
    "\n",
    "# Figures config\n",
    "legend_font_size = 5\n",
    "text_font_size = 5\n",
    "\n",
    "fig_roc_size = figsize_in_mm(48, 37)\n",
    "\n",
    "fig_surv_size = figsize_in_mm(56, 36)\n",
    "\n",
    "fig_sc_size = figsize_in_mm(100, 40)\n",
    "fig_sc_bar_width = 0.2\n",
    "\n",
    "#\n",
    "\n",
    "if cut_input_params:\n",
    "    name_suffix = \"-cut\"\n",
    "else:\n",
    "    name_suffix = \"-full\"\n",
    "\n",
    "mut_vec_len_label = \"mut_vec_len\"\n",
    "clf_params_label = \"clf_params\"\n",
    "model_name_label = \"plot_label\"\n",
    "\n",
    "m_num = len(cfg.configurations)\n",
    "\n",
    "model_colors = {n[model_name_label]: f\"C{i}\" for i, n in enumerate(cfg.configurations)}\n",
    "model_colors_surv_lines = {}\n",
    "i = 0\n",
    "for n in cfg.configurations:\n",
    "    lines_c = [plt.color_sequences[\"tab20\"][i + 1], plt.color_sequences[\"tab20\"][i]]\n",
    "    model_colors_surv_lines[n[model_name_label]] = lines_c\n",
    "    i += 2\n",
    "\n",
    "fig_roc = plt.figure(figsize=fig_roc_size, layout=\"constrained\")\n",
    "ax_roc = fig_roc.add_subplot()\n",
    "\n",
    "test_scores_str = \"\"\n",
    "test_scores_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\"]\n",
    "test_scores = {}\n",
    "\n",
    "surv_ylabel = True\n",
    "\n",
    "#\n",
    "d = []\n",
    "#\n",
    "\n",
    "for config in tqdm(cfg.configurations, leave=False):\n",
    "    dimension_of_embedding_vectors = config[mut_vec_len_label]\n",
    "    clf_params = config[clf_params_label]\n",
    "    model_name = config[model_name_label]\n",
    "\n",
    "    #\n",
    "    # config[\"with_mutations\"] = False\n",
    "    #\n",
    "\n",
    "    train_data, train_y, test_data, test_y, test_pfs, _, _, _, val_data, val_y, val_pfs = ds.transforming_Braun_dataset(\n",
    "        config,\n",
    "        dimension_of_embedding_vectors=dimension_of_embedding_vectors,\n",
    "        cut_input_params=cut_input_params,\n",
    "        validation_features_file=validation_clinical_features_file,\n",
    "    )\n",
    "\n",
    "    #\n",
    "    d.append([train_data, train_y, test_data, test_y, test_pfs, val_data, val_y, val_pfs])\n",
    "    #\n",
    "\n",
    "    input_data = test_data\n",
    "    input_y = test_y\n",
    "    input_pfs = test_pfs\n",
    "\n",
    "    # Load SVM model and run validation data\n",
    "\n",
    "    clf_file = svm_model_file(model_name, cut_input_params)\n",
    "    with open(clf_file, \"rb\") as f:\n",
    "        clf = pickle.load(f)\n",
    "\n",
    "    svm_linear_preds = clf.predict(input_data)\n",
    "    probs = clf.predict_proba(input_data)\n",
    "    probSV = [i[1] for i in probs]\n",
    "    svm_prob = pd.DataFrame(probSV)\n",
    "\n",
    "    probs = svm_prob.values.flatten()\n",
    "    fpr, tpr, _ = roc_curve(input_y, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    prec = precision_score(input_y, svm_linear_preds)\n",
    "    rec = recall_score(input_y, svm_linear_preds)\n",
    "    f1 = f1_score(input_y, svm_linear_preds)\n",
    "    acc = accuracy_score(input_y, svm_linear_preds)\n",
    "    rauc = roc_auc_score(input_y, probs)\n",
    "\n",
    "    ba = balanced_accuracy_score(input_y, svm_linear_preds)\n",
    "\n",
    "    test_scores_str += f\"{model_name}:\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[0]}: {acc:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[1]}: {prec:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[2]}: {rec:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[3]}: {f1:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[4]}: {rauc:.3f}\\n\"\n",
    "    test_scores_str += f\"    _MEAN_: {(acc + prec + rec + f1 + rauc) / 5:.3f}\\n\"\n",
    "    test_scores_str += f\"    BA: {ba:.3f}\\n\"\n",
    "\n",
    "    test_scores[model_name] = {}\n",
    "    test_scores[model_name][test_scores_names[0]] = acc\n",
    "    test_scores[model_name][test_scores_names[1]] = prec\n",
    "    test_scores[model_name][test_scores_names[2]] = rec\n",
    "    test_scores[model_name][test_scores_names[3]] = f1\n",
    "    test_scores[model_name][test_scores_names[4]] = rauc\n",
    "\n",
    "    _ = ax_roc.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        label=f\"{model_name} (AUC={roc_auc:.3f})\",\n",
    "        color=model_colors[model_name],\n",
    "        linewidth=1,\n",
    "    )\n",
    "\n",
    "    # Response prediction\n",
    "    time_event = input_pfs.reset_index(drop=True)\n",
    "    # True only for this particular case\n",
    "    val_pfs_cnsr = pd.Series([0] * input_data.shape[0])\n",
    "    #\n",
    "    censoring = val_pfs_cnsr.reset_index(drop=True)\n",
    "    # censoring = pd.Series([1 if p == -1 else 0 for p in svm_linear_preds])\n",
    "    labx = pd.Series(\n",
    "        [\"Responders\" if y == 1 else \"Non-responders\" for y in svm_linear_preds]\n",
    "    )\n",
    "    # labx = pd.Series([\"Responders\" if y == 1 else \"Non-responders\" for y in test_y])\n",
    "    surv_pred = km.fit(time_event, censoring, labx)\n",
    "    logrank_p = surv_pred[\"logrank_P\"]\n",
    "    fig_surv, ax_surv = km.plot(\n",
    "        surv_pred,\n",
    "        figsize=fig_surv_size,\n",
    "        fontsize=main_font_size,\n",
    "        cmap=model_colors_surv_lines[model_name],\n",
    "    )\n",
    "    # old_title = ax_surv.get_title()\n",
    "    # p_value_str = old_title.replace(\"Survival function\\n\", \"\")\n",
    "    ax_surv.set_title(None)\n",
    "    ax_surv.text(\n",
    "        0.85,\n",
    "        0.3,\n",
    "        f\"Logrank P-value:\\n{logrank_p:.3e}\",\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        transform=ax_surv.transAxes,\n",
    "        fontsize=text_font_size,\n",
    "    )\n",
    "    ax_surv.set_title(f\"{model_name}\")\n",
    "    # ax_surv.set_title(\n",
    "    #     old_title.replace(\"Survival function\", f\"{model_name}\"),\n",
    "    #     {\"fontsize\": label_font_size},\n",
    "    # )\n",
    "    ax_surv.tick_params(length=3.5)\n",
    "    ax_surv.set_xlabel(\"Time (months)\")\n",
    "    if surv_ylabel:\n",
    "        ax_surv.set_ylabel(\"Probability of response\")\n",
    "        surv_ylabel = False\n",
    "    ax_surv.legend(\n",
    "        handlelength=0.8,\n",
    "        handletextpad=0.5,\n",
    "        labelspacing=0.8,\n",
    "        fontsize=legend_font_size,\n",
    "        loc=\"upper right\",\n",
    "        # loc=\"upper center\",\n",
    "        # bbox_to_anchor=(0.5, -0.4),\n",
    "    )\n",
    "    for fmt in [\"pdf\", \"png\"]:\n",
    "        fig_surv.savefig(\n",
    "            validation_dir.joinpath(\n",
    "                f\"val-response-prediction-{model_name.replace(' ', '_')}{name_suffix}.{fmt}\"\n",
    "            ),\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0.03,\n",
    "            dpi=600,\n",
    "        )\n",
    "\n",
    "# ROC\n",
    "ax_roc.set_xlabel(\"False Positive Rate\")\n",
    "ax_roc.set_ylabel(\"True Positive Rate\")\n",
    "# ax_roc.set_title(\"ROC Curves for different levels\", {\"fontsize\": label_font_size})\n",
    "ax_roc.legend(loc=\"lower right\", fontsize=legend_font_size)\n",
    "for fmt in [\"pdf\", \"png\"]:\n",
    "    fig_roc.savefig(\n",
    "        validation_dir.joinpath(f\"val-roc-curves{name_suffix}.{fmt}\"), dpi=600\n",
    "    )\n",
    "\n",
    "# Print and save scores to a file\n",
    "print(test_scores_str)\n",
    "with open(validation_dir.joinpath(f\"val-scores{name_suffix}.txt\"), \"w\") as f:\n",
    "    f.write(test_scores_str)\n",
    "\n",
    "# Scores figure\n",
    "m_names = list(test_scores.keys())\n",
    "s_num = len(test_scores_names)\n",
    "x = np.arange(s_num)\n",
    "b_pos = np.arange(\n",
    "    fig_sc_bar_width / 2 * (1 - m_num),\n",
    "    fig_sc_bar_width / 2 * (m_num - 1),\n",
    "    fig_sc_bar_width,\n",
    ")\n",
    "fig_sc = plt.figure(figsize=fig_sc_size, layout=\"constrained\")\n",
    "ax_sc = fig_sc.add_subplot()\n",
    "for n, p in zip(m_names, b_pos):\n",
    "    _ = ax_sc.bar(\n",
    "        x + p,\n",
    "        list(test_scores[n].values()),\n",
    "        width=fig_sc_bar_width,\n",
    "        label=n,\n",
    "        color=model_colors[n],\n",
    "        zorder=10,\n",
    "    )\n",
    "ax_sc.set_xticks(x, test_scores_names)\n",
    "ax_sc.grid(axis=\"y\")\n",
    "_ = ax_sc.legend(\n",
    "    fontsize=legend_font_size, ncols=m_num, loc=\"lower right\", bbox_to_anchor=(1, 1)\n",
    ")\n",
    "for fmt in [\"pdf\", \"png\"]:\n",
    "    fig_sc.savefig(validation_dir.joinpath(f\"val-scores{name_suffix}.{fmt}\"), dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "org_cf = pd.read_csv(\"../data/Braun_2020_ALL_UNIQUE_final_reduced.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[2][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0][5].loc[d[0][9].isin(d[0][8])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0][0].equals(d[0][5].loc[d[0][9].isin(d[0][8])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0][8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0][9].loc[d[0][9].isin(d[0][8])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[0][9].loc[org_cf[\"TrainTestStatus\"] == \"Train\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svn-opti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
