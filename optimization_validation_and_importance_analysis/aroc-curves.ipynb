{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import _config as cfg  # noqa: E402\n",
    "import DataSets_validation as ds  # noqa: E402\n",
    "import kaplanmeier as km  # noqa: E402\n",
    "import matplotlib.pyplot as plt  # noqa: E402\n",
    "import numpy as np  # noqa: E402\n",
    "import pandas as pd  # noqa: E402\n",
    "from imblearn.over_sampling import SMOTE  # noqa: E402\n",
    "from sklearnex import patch_sklearn  # noqa: E402\n",
    "\n",
    "patch_sklearn()\n",
    "\n",
    "from sklearn import svm  # noqa: E402\n",
    "from sklearn.metrics import (  # noqa: E402\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from tqdm.notebook import tqdm  # noqa: E402\n",
    "\n",
    "yeloh_seed = 2137\n",
    "\n",
    "random.seed(1024)\n",
    "np.random.seed(1024)\n",
    "\n",
    "results_dir = Path(\"Results\")\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "validation_dir = results_dir / \"validation\"\n",
    "validation_dir.mkdir(parents=True, exist_ok=True)\n",
    "validation_clinical_features_file = \"/data/teamgdansk/katy-variants/validation-datasets/KATY_Edin_cohort_clinical_features.csv\"\n",
    "\n",
    "\n",
    "def svm_model_file(model_name, cut_input_params=False):\n",
    "    model_name = model_name.replace(\" \", \"_\")\n",
    "    if cut_input_params:\n",
    "        name_suffix = \"-cut\"\n",
    "    else:\n",
    "        name_suffix = \"-full\"\n",
    "    return results_dir / f\"svm-model-{model_name}{name_suffix}.pkl\"\n",
    "\n",
    "\n",
    "# Matplotlib config\n",
    "plt.rcdefaults()\n",
    "\n",
    "main_font_size = 6\n",
    "label_font_size = 10\n",
    "\n",
    "axis_color = \"0.15\"\n",
    "\n",
    "plt.rcParams[\"text.color\"] = axis_color\n",
    "plt.rcParams[\"axes.labelcolor\"] = axis_color\n",
    "plt.rcParams[\"xtick.color\"] = axis_color\n",
    "plt.rcParams[\"ytick.color\"] = axis_color\n",
    "plt.rcParams[\"axes.edgecolor\"] = axis_color\n",
    "\n",
    "plt.rcParams[\"font.size\"] = main_font_size\n",
    "plt.rcParams[\"axes.labelsize\"] = main_font_size\n",
    "plt.rcParams[\"axes.titlesize\"] = main_font_size\n",
    "plt.rcParams[\"xtick.labelsize\"] = main_font_size\n",
    "plt.rcParams[\"ytick.labelsize\"] = main_font_size\n",
    "plt.rcParams[\"legend.fontsize\"] = main_font_size\n",
    "plt.rcParams[\"figure.labelsize\"] = main_font_size\n",
    "plt.rcParams[\"figure.titlesize\"] = main_font_size\n",
    "\n",
    "plt.rcParams[\"axes.spines.top\"] = False\n",
    "plt.rcParams[\"axes.spines.right\"] = False\n",
    "\n",
    "plt.rcParams[\"savefig.transparent\"] = True\n",
    "\n",
    "plt.rcParams[\"pdf.fonttype\"] = 42\n",
    "plt.rcParams[\"ps.fonttype\"] = 42\n",
    "plt.rcParams[\"svg.fonttype\"] = \"none\"\n",
    "\n",
    "\n",
    "def figsize_in_mm(width, height):\n",
    "    mm = 1 / 25.4  # millimeters in inches\n",
    "    return (width * mm, height * mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_train_test(train_data, train_y, test_data, verbose=False, clf_kwargs={}):\n",
    "    sm = SMOTE(random_state=432)\n",
    "    train_data, train_y = sm.fit_resample(train_data, train_y)\n",
    "    clf = svm.SVC(probability=True, random_state=yeloh_seed, **clf_kwargs)\n",
    "    clf = clf.fit(train_data, train_y)\n",
    "\n",
    "    predictions = clf.predict(test_data)\n",
    "    probs = clf.predict_proba(test_data)\n",
    "    probSV = [i[1] for i in probs]\n",
    "    if verbose:\n",
    "        print(probSV)\n",
    "    new_pd = pd.DataFrame(probSV)\n",
    "    return clf, predictions, new_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CF files analysis:\")\n",
    "\n",
    "cf_core_cols = [\"PFS\"]\n",
    "\n",
    "org_cf = pd.read_csv(\"../data/Braun_2020_ALL_UNIQUE_final_reduced.csv\")\n",
    "val_cf = pd.read_csv(validation_clinical_features_file)\n",
    "\n",
    "org_cf_cols = set(org_cf.columns)\n",
    "val_cf_cols = set(val_cf.columns)\n",
    "\n",
    "new_cols = val_cf_cols - org_cf_cols\n",
    "new_cols_n = len(new_cols)\n",
    "if new_cols_n > 0:\n",
    "    print(f\"  Validation CF file extra cols N: {new_cols_n}\")\n",
    "new_cols = org_cf_cols - val_cf_cols\n",
    "new_cols_n = len(new_cols)\n",
    "if new_cols_n > 0:\n",
    "    print(f\"  Original CF file extra cols N: {new_cols_n}\")\n",
    "\n",
    "common_cols = sorted(org_cf_cols.intersection(val_cf_cols))\n",
    "if len(org_cf_cols) != len(val_cf_cols):\n",
    "    print(f\"\\n  Common columns: {common_cols}\")\n",
    "\n",
    "first = True\n",
    "for c in common_cols:\n",
    "    if org_cf[c].dtype != val_cf[c].dtype:\n",
    "        if first:\n",
    "            print(\"\\n  Different dtype for:\")\n",
    "            first = False\n",
    "        print(f\"    {c}\")\n",
    "\n",
    "print(\"\\n\\nMutation files analysis:\")\n",
    "\n",
    "mut_analysis_core_cols = [\"Chromosome\", \"Start_position\", \"End_position\"]\n",
    "\n",
    "for config in cfg.configurations:\n",
    "    model_name = config[\"plot_label\"]\n",
    "    print()\n",
    "    print(f\"Mut `{model_name}` files:\")\n",
    "\n",
    "    hs_cols = config[\"HS_features\"]\n",
    "    mut_analysis_cols = mut_analysis_core_cols + hs_cols + [\"contig\", \"SUBJID\"]\n",
    "\n",
    "    org_mut = pd.read_csv(config[\"contig_file\"], sep=\"\\t\", low_memory=False)\n",
    "    val_mut = pd.read_csv(config[\"validation_contig_file\"], sep=\"\\t\", low_memory=False)\n",
    "\n",
    "    org_mut_cols = set(org_mut.columns)\n",
    "    val_mut_cols = set(val_mut.columns)\n",
    "\n",
    "    first = True\n",
    "    for ma_c in mut_analysis_cols:\n",
    "        if ma_c not in val_mut_cols:\n",
    "            if first:\n",
    "                print(\"\\n  Required column not in the table:\")\n",
    "                first = False\n",
    "            print(f\"    {ma_c}\")\n",
    "\n",
    "    print(val_mut[mut_analysis_core_cols + hs_cols].dropna())\n",
    "\n",
    "    new_cols = val_mut_cols - org_mut_cols\n",
    "    new_cols_n = len(new_cols)\n",
    "    if new_cols_n > 0:\n",
    "        print(f\"  Validation Mut file extra cols N: {new_cols_n}\")\n",
    "    new_cols = org_mut_cols - val_mut_cols\n",
    "    new_cols_n = len(new_cols)\n",
    "    if new_cols_n > 0:\n",
    "        print(f\"  Original Mut file extra cols N: {new_cols_n}\")\n",
    "\n",
    "    common_cols = sorted(org_mut_cols.intersection(val_mut_cols))\n",
    "    if len(org_mut_cols) != len(val_mut_cols):\n",
    "        print(f\"\\n  Common columns: {common_cols}\")\n",
    "\n",
    "    first = True\n",
    "    for c in common_cols:\n",
    "        if org_mut[c].dtype != val_mut[c].dtype:\n",
    "            if first:\n",
    "                print(\"\\n  Different dtype for:\")\n",
    "                first = False\n",
    "            print(f\"    {c}\")\n",
    "            if c in mut_analysis_cols:\n",
    "                print(\"      (in required)\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "# Set `True` if you want to use only age, gender, and mutation data:\n",
    "cut_input_params = True\n",
    "\n",
    "# Figures config\n",
    "legend_font_size = 5\n",
    "text_font_size = 5\n",
    "\n",
    "fig_roc_size = figsize_in_mm(48, 37)\n",
    "\n",
    "fig_surv_size = figsize_in_mm(56, 36)\n",
    "\n",
    "fig_sc_size = figsize_in_mm(100, 40)\n",
    "fig_sc_bar_width = 0.2\n",
    "\n",
    "#\n",
    "\n",
    "if cut_input_params:\n",
    "    name_suffix = \"-cut\"\n",
    "else:\n",
    "    name_suffix = \"-full\"\n",
    "\n",
    "mut_vec_len_label = \"mut_vec_len\"\n",
    "clf_params_label = \"clf_params\"\n",
    "model_name_label = \"plot_label\"\n",
    "\n",
    "m_num = len(cfg.configurations)\n",
    "\n",
    "model_colors = {n[model_name_label]: f\"C{i}\" for i, n in enumerate(cfg.configurations)}\n",
    "model_colors_surv_lines = {}\n",
    "i = 0\n",
    "for n in cfg.configurations:\n",
    "    lines_c = [plt.color_sequences[\"tab20\"][i + 1], plt.color_sequences[\"tab20\"][i]]\n",
    "    model_colors_surv_lines[n[model_name_label]] = lines_c\n",
    "    i += 2\n",
    "\n",
    "fig_roc = plt.figure(figsize=fig_roc_size, layout=\"constrained\")\n",
    "ax_roc = fig_roc.add_subplot()\n",
    "\n",
    "test_scores_str = \"\"\n",
    "test_scores_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\"]\n",
    "test_scores = {}\n",
    "\n",
    "surv_ylabel = True\n",
    "\n",
    "for config in tqdm(cfg.configurations, leave=False):\n",
    "    dimension_of_embedding_vectors = config[mut_vec_len_label]\n",
    "    clf_params = config[clf_params_label]\n",
    "    model_name = config[model_name_label]\n",
    "\n",
    "    train_data, train_y, test_data, test_y, test_pfs, _, _, test_pfs_cnsr, _, _, _ = (\n",
    "        ds.transforming_Braun_dataset(\n",
    "            config,\n",
    "            dimension_of_embedding_vectors=dimension_of_embedding_vectors,\n",
    "            cut_input_params=cut_input_params,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    model, svm_linear_preds, svm_prob = svm_train_test(\n",
    "        train_data, train_y, test_data, clf_kwargs=clf_params\n",
    "    )\n",
    "\n",
    "    probs = svm_prob.values.flatten()\n",
    "    fpr, tpr, _ = roc_curve(test_y, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    prec = precision_score(test_y, svm_linear_preds)\n",
    "    rec = recall_score(test_y, svm_linear_preds)\n",
    "    f1 = f1_score(test_y, svm_linear_preds)\n",
    "    acc = accuracy_score(test_y, svm_linear_preds)\n",
    "    rauc = roc_auc_score(test_y, probs)\n",
    "\n",
    "    ba = balanced_accuracy_score(test_y, svm_linear_preds)\n",
    "\n",
    "    test_scores_str += f\"{model_name}:\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[0]}: {acc:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[1]}: {prec:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[2]}: {rec:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[3]}: {f1:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[4]}: {rauc:.3f}\\n\"\n",
    "    test_scores_str += f\"    _MEAN_: {(acc + prec + rec + f1 + rauc) / 5:.3f}\\n\"\n",
    "    test_scores_str += f\"    BA: {ba:.3f}\\n\"\n",
    "\n",
    "    test_scores[model_name] = {}\n",
    "    test_scores[model_name][test_scores_names[0]] = acc\n",
    "    test_scores[model_name][test_scores_names[1]] = prec\n",
    "    test_scores[model_name][test_scores_names[2]] = rec\n",
    "    test_scores[model_name][test_scores_names[3]] = f1\n",
    "    test_scores[model_name][test_scores_names[4]] = rauc\n",
    "\n",
    "    _ = ax_roc.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        label=f\"{model_name} (AUC={roc_auc:.3f})\",\n",
    "        color=model_colors[model_name],\n",
    "        linewidth=1,\n",
    "    )\n",
    "\n",
    "    with open(svm_model_file(model_name, cut_input_params), \"wb\") as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # Response prediction\n",
    "    time_event = test_pfs.reset_index(drop=True)\n",
    "    censoring = test_pfs_cnsr.reset_index(drop=True)\n",
    "    # censoring = pd.Series([1 if p == -1 else 0 for p in svm_linear_preds])\n",
    "    labx = pd.Series(\n",
    "        [\"Responders\" if y == 1 else \"Non-responders\" for y in svm_linear_preds]\n",
    "    )\n",
    "    # labx = pd.Series([\"Responders\" if y == 1 else \"Non-responders\" for y in test_y])\n",
    "    surv_pred = km.fit(time_event, censoring, labx)\n",
    "    logrank_p = surv_pred[\"logrank_P\"]\n",
    "    fig_surv, ax_surv = km.plot(\n",
    "        surv_pred,\n",
    "        figsize=fig_surv_size,\n",
    "        fontsize=main_font_size,\n",
    "        cmap=model_colors_surv_lines[model_name],\n",
    "    )\n",
    "    # old_title = ax_surv.get_title()\n",
    "    # p_value_str = old_title.replace(\"Survival function\\n\", \"\")\n",
    "    ax_surv.set_title(None)\n",
    "    ax_surv.text(\n",
    "        0.85,\n",
    "        0.3,\n",
    "        f\"Logrank P-value:\\n{logrank_p:.3e}\",\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        transform=ax_surv.transAxes,\n",
    "        fontsize=text_font_size,\n",
    "    )\n",
    "    ax_surv.set_title(f\"{model_name}\")\n",
    "    # ax_surv.set_title(\n",
    "    #     old_title.replace(\"Survival function\", f\"{model_name}\"),\n",
    "    #     {\"fontsize\": label_font_size},\n",
    "    # )\n",
    "    ax_surv.tick_params(length=3.5)\n",
    "    ax_surv.set_xlabel(\"Time (months)\")\n",
    "    if surv_ylabel:\n",
    "        ax_surv.set_ylabel(\"Probability of response\")\n",
    "        surv_ylabel = False\n",
    "    ax_surv.legend(\n",
    "        handlelength=0.8,\n",
    "        handletextpad=0.5,\n",
    "        labelspacing=0.8,\n",
    "        fontsize=legend_font_size,\n",
    "        loc=\"upper right\",\n",
    "        # loc=\"upper center\",\n",
    "        # bbox_to_anchor=(0.5, -0.4),\n",
    "    )\n",
    "    for fmt in [\"pdf\", \"png\"]:\n",
    "        fig_surv.savefig(\n",
    "            results_dir.joinpath(\n",
    "                f\"response-prediction-{model_name.replace(' ', '_')}{name_suffix}.{fmt}\"\n",
    "            ),\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0.03,\n",
    "            dpi=600,\n",
    "        )\n",
    "\n",
    "# ROC\n",
    "ax_roc.set_xlabel(\"False Positive Rate\")\n",
    "ax_roc.set_ylabel(\"True Positive Rate\")\n",
    "# ax_roc.set_title(\"ROC Curves for different levels\", {\"fontsize\": label_font_size})\n",
    "ax_roc.legend(loc=\"lower right\", fontsize=legend_font_size)\n",
    "for fmt in [\"pdf\", \"png\"]:\n",
    "    fig_roc.savefig(results_dir.joinpath(f\"roc-curves{name_suffix}.{fmt}\"), dpi=600)\n",
    "\n",
    "# Print and save scores to a file\n",
    "print(test_scores_str)\n",
    "with open(results_dir.joinpath(f\"scores{name_suffix}.txt\"), \"w\") as f:\n",
    "    f.write(test_scores_str)\n",
    "\n",
    "# Scores figure\n",
    "m_names = list(test_scores.keys())\n",
    "s_num = len(test_scores_names)\n",
    "x = np.arange(s_num)\n",
    "b_pos = np.arange(\n",
    "    fig_sc_bar_width / 2 * (1 - m_num),\n",
    "    fig_sc_bar_width / 2 * (m_num - 1),\n",
    "    fig_sc_bar_width,\n",
    ")\n",
    "fig_sc = plt.figure(figsize=fig_sc_size, layout=\"constrained\")\n",
    "ax_sc = fig_sc.add_subplot()\n",
    "for n, p in zip(m_names, b_pos):\n",
    "    _ = ax_sc.bar(\n",
    "        x + p,\n",
    "        list(test_scores[n].values()),\n",
    "        width=fig_sc_bar_width,\n",
    "        label=n,\n",
    "        color=model_colors[n],\n",
    "        zorder=10,\n",
    "    )\n",
    "ax_sc.set_xticks(x, test_scores_names)\n",
    "ax_sc.grid(axis=\"y\")\n",
    "_ = ax_sc.legend(\n",
    "    fontsize=legend_font_size, ncols=m_num, loc=\"lower right\", bbox_to_anchor=(1, 1)\n",
    ")\n",
    "for fmt in [\"pdf\", \"png\"]:\n",
    "    fig_sc.savefig(results_dir.joinpath(f\"scores{name_suffix}.{fmt}\"), dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "# Set `True` if you want to use only age, gender, and mutation data:\n",
    "cut_input_params = True\n",
    "\n",
    "# Figures config\n",
    "legend_font_size = 5\n",
    "text_font_size = 5\n",
    "\n",
    "fig_roc_size = figsize_in_mm(48, 37)\n",
    "\n",
    "fig_surv_size = figsize_in_mm(56, 36)\n",
    "\n",
    "fig_sc_size = figsize_in_mm(100, 40)\n",
    "fig_sc_bar_width = 0.2\n",
    "\n",
    "#\n",
    "\n",
    "if cut_input_params:\n",
    "    name_suffix = \"-cut\"\n",
    "else:\n",
    "    name_suffix = \"-full\"\n",
    "\n",
    "mut_vec_len_label = \"mut_vec_len\"\n",
    "clf_params_label = \"clf_params\"\n",
    "model_name_label = \"plot_label\"\n",
    "\n",
    "m_num = len(cfg.configurations)\n",
    "\n",
    "model_colors = {n[model_name_label]: f\"C{i}\" for i, n in enumerate(cfg.configurations)}\n",
    "model_colors_surv_lines = {}\n",
    "i = 0\n",
    "for n in cfg.configurations:\n",
    "    lines_c = [plt.color_sequences[\"tab20\"][i + 1], plt.color_sequences[\"tab20\"][i]]\n",
    "    model_colors_surv_lines[n[model_name_label]] = lines_c\n",
    "    i += 2\n",
    "\n",
    "fig_roc = plt.figure(figsize=fig_roc_size, layout=\"constrained\")\n",
    "ax_roc = fig_roc.add_subplot()\n",
    "\n",
    "test_scores_str = \"\"\n",
    "test_scores_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\"]\n",
    "test_scores = {}\n",
    "\n",
    "surv_ylabel = True\n",
    "\n",
    "#\n",
    "d = []\n",
    "#\n",
    "\n",
    "for config in tqdm(cfg.configurations, leave=False):\n",
    "    dimension_of_embedding_vectors = config[mut_vec_len_label]\n",
    "    clf_params = config[clf_params_label]\n",
    "    model_name = config[model_name_label]\n",
    "\n",
    "    train_data, train_y, test_data, test_y, test_pfs, _, _, _, val_data, val_y, val_pfs = (\n",
    "        ds.transforming_Braun_dataset(\n",
    "            config,\n",
    "            dimension_of_embedding_vectors=dimension_of_embedding_vectors,\n",
    "            cut_input_params=cut_input_params, validation_features_file=validation_clinical_features_file\n",
    "        )\n",
    "    )\n",
    "\n",
    "    #\n",
    "    d.append([train_data, train_y, test_data, test_y, test_pfs, val_data, val_y, val_pfs])\n",
    "    #\n",
    "\n",
    "#     model, svm_linear_preds, svm_prob = svm_train_test(\n",
    "#         train_data, train_y, test_data, clf_kwargs=clf_params\n",
    "#     )\n",
    "\n",
    "#     probs = svm_prob.values.flatten()\n",
    "#     fpr, tpr, _ = roc_curve(test_y, probs)\n",
    "#     roc_auc = auc(fpr, tpr)\n",
    "\n",
    "#     prec = precision_score(test_y, svm_linear_preds)\n",
    "#     rec = recall_score(test_y, svm_linear_preds)\n",
    "#     f1 = f1_score(test_y, svm_linear_preds)\n",
    "#     acc = accuracy_score(test_y, svm_linear_preds)\n",
    "#     rauc = roc_auc_score(test_y, probs)\n",
    "\n",
    "#     ba = balanced_accuracy_score(test_y, svm_linear_preds)\n",
    "\n",
    "#     test_scores_str += f\"{model_name}:\\n\"\n",
    "#     test_scores_str += f\"  {test_scores_names[0]}: {acc:.3f}\\n\"\n",
    "#     test_scores_str += f\"  {test_scores_names[1]}: {prec:.3f}\\n\"\n",
    "#     test_scores_str += f\"  {test_scores_names[2]}: {rec:.3f}\\n\"\n",
    "#     test_scores_str += f\"  {test_scores_names[3]}: {f1:.3f}\\n\"\n",
    "#     test_scores_str += f\"  {test_scores_names[4]}: {rauc:.3f}\\n\"\n",
    "#     test_scores_str += f\"    _MEAN_: {(acc + prec + rec + f1 + rauc) / 5:.3f}\\n\"\n",
    "#     test_scores_str += f\"    BA: {ba:.3f}\\n\"\n",
    "\n",
    "#     test_scores[model_name] = {}\n",
    "#     test_scores[model_name][test_scores_names[0]] = acc\n",
    "#     test_scores[model_name][test_scores_names[1]] = prec\n",
    "#     test_scores[model_name][test_scores_names[2]] = rec\n",
    "#     test_scores[model_name][test_scores_names[3]] = f1\n",
    "#     test_scores[model_name][test_scores_names[4]] = rauc\n",
    "\n",
    "#     _ = ax_roc.plot(\n",
    "#         fpr,\n",
    "#         tpr,\n",
    "#         label=f\"{model_name} (AUC={roc_auc:.3f})\",\n",
    "#         color=model_colors[model_name],\n",
    "#         linewidth=1,\n",
    "#     )\n",
    "\n",
    "#     with open(svm_model_file(model_name, cut_input_params), \"wb\") as f:\n",
    "#         pickle.dump(model, f)\n",
    "\n",
    "#     # Response prediction\n",
    "#     time_event = test_pfs.reset_index(drop=True)\n",
    "#     censoring = test_pfs_cnsr.reset_index(drop=True)\n",
    "#     # censoring = pd.Series([1 if p == -1 else 0 for p in svm_linear_preds])\n",
    "#     labx = pd.Series(\n",
    "#         [\"Responders\" if y == 1 else \"Non-responders\" for y in svm_linear_preds]\n",
    "#     )\n",
    "#     # labx = pd.Series([\"Responders\" if y == 1 else \"Non-responders\" for y in test_y])\n",
    "#     surv_pred = km.fit(time_event, censoring, labx)\n",
    "#     logrank_p = surv_pred[\"logrank_P\"]\n",
    "#     fig_surv, ax_surv = km.plot(\n",
    "#         surv_pred,\n",
    "#         figsize=fig_surv_size,\n",
    "#         fontsize=main_font_size,\n",
    "#         cmap=model_colors_surv_lines[model_name],\n",
    "#     )\n",
    "#     # old_title = ax_surv.get_title()\n",
    "#     # p_value_str = old_title.replace(\"Survival function\\n\", \"\")\n",
    "#     ax_surv.set_title(None)\n",
    "#     ax_surv.text(\n",
    "#         0.85,\n",
    "#         0.3,\n",
    "#         f\"Logrank P-value:\\n{logrank_p:.3e}\",\n",
    "#         ha=\"right\",\n",
    "#         va=\"bottom\",\n",
    "#         transform=ax_surv.transAxes,\n",
    "#         fontsize=text_font_size,\n",
    "#     )\n",
    "#     ax_surv.set_title(f\"{model_name}\")\n",
    "#     # ax_surv.set_title(\n",
    "#     #     old_title.replace(\"Survival function\", f\"{model_name}\"),\n",
    "#     #     {\"fontsize\": label_font_size},\n",
    "#     # )\n",
    "#     ax_surv.tick_params(length=3.5)\n",
    "#     ax_surv.set_xlabel(\"Time (months)\")\n",
    "#     if surv_ylabel:\n",
    "#         ax_surv.set_ylabel(\"Probability of response\")\n",
    "#         surv_ylabel = False\n",
    "#     ax_surv.legend(\n",
    "#         handlelength=0.8,\n",
    "#         handletextpad=0.5,\n",
    "#         labelspacing=0.8,\n",
    "#         fontsize=legend_font_size,\n",
    "#         loc=\"upper right\",\n",
    "#         # loc=\"upper center\",\n",
    "#         # bbox_to_anchor=(0.5, -0.4),\n",
    "#     )\n",
    "#     for fmt in [\"pdf\", \"png\"]:\n",
    "#         fig_surv.savefig(\n",
    "#             results_dir.joinpath(\n",
    "#                 f\"response-prediction-{model_name.replace(' ', '_')}{name_suffix}.{fmt}\"\n",
    "#             ),\n",
    "#             bbox_inches=\"tight\",\n",
    "#             pad_inches=0.03,\n",
    "#             dpi=600,\n",
    "#         )\n",
    "\n",
    "# # ROC\n",
    "# ax_roc.set_xlabel(\"False Positive Rate\")\n",
    "# ax_roc.set_ylabel(\"True Positive Rate\")\n",
    "# # ax_roc.set_title(\"ROC Curves for different levels\", {\"fontsize\": label_font_size})\n",
    "# ax_roc.legend(loc=\"lower right\", fontsize=legend_font_size)\n",
    "# for fmt in [\"pdf\", \"png\"]:\n",
    "#     fig_roc.savefig(results_dir.joinpath(f\"roc-curves{name_suffix}.{fmt}\"), dpi=600)\n",
    "\n",
    "# # Print and save scores to a file\n",
    "# print(test_scores_str)\n",
    "# with open(results_dir.joinpath(f\"scores{name_suffix}.txt\"), \"w\") as f:\n",
    "#     f.write(test_scores_str)\n",
    "\n",
    "# # Scores figure\n",
    "# m_names = list(test_scores.keys())\n",
    "# s_num = len(test_scores_names)\n",
    "# x = np.arange(s_num)\n",
    "# b_pos = np.arange(\n",
    "#     fig_sc_bar_width / 2 * (1 - m_num),\n",
    "#     fig_sc_bar_width / 2 * (m_num - 1),\n",
    "#     fig_sc_bar_width,\n",
    "# )\n",
    "# fig_sc = plt.figure(figsize=fig_sc_size, layout=\"constrained\")\n",
    "# ax_sc = fig_sc.add_subplot()\n",
    "# for n, p in zip(m_names, b_pos):\n",
    "#     _ = ax_sc.bar(\n",
    "#         x + p,\n",
    "#         list(test_scores[n].values()),\n",
    "#         width=fig_sc_bar_width,\n",
    "#         label=n,\n",
    "#         color=model_colors[n],\n",
    "#         zorder=10,\n",
    "#     )\n",
    "# ax_sc.set_xticks(x, test_scores_names)\n",
    "# ax_sc.grid(axis=\"y\")\n",
    "# _ = ax_sc.legend(\n",
    "#     fontsize=legend_font_size, ncols=m_num, loc=\"lower right\", bbox_to_anchor=(1, 1)\n",
    "# )\n",
    "# for fmt in [\"pdf\", \"png\"]:\n",
    "#     fig_sc.savefig(results_dir.joinpath(f\"scores{name_suffix}.{fmt}\"), dpi=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config\n",
    "\n",
    "# Set `True` if you want to use only age, gender, and mutation data:\n",
    "cut_input_params = True\n",
    "\n",
    "# Figures config\n",
    "legend_font_size = 5\n",
    "text_font_size = 5\n",
    "\n",
    "fig_roc_size = figsize_in_mm(48, 37)\n",
    "\n",
    "fig_surv_size = figsize_in_mm(56, 36)\n",
    "\n",
    "fig_sc_size = figsize_in_mm(100, 40)\n",
    "fig_sc_bar_width = 0.2\n",
    "\n",
    "#\n",
    "\n",
    "if cut_input_params:\n",
    "    name_suffix = \"-cut\"\n",
    "else:\n",
    "    name_suffix = \"-full\"\n",
    "\n",
    "mut_vec_len_label = \"mut_vec_len\"\n",
    "clf_params_label = \"clf_params\"\n",
    "model_name_label = \"plot_label\"\n",
    "\n",
    "m_num = len(cfg.configurations)\n",
    "\n",
    "model_colors = {n[model_name_label]: f\"C{i}\" for i, n in enumerate(cfg.configurations)}\n",
    "model_colors_surv_lines = {}\n",
    "i = 0\n",
    "for n in cfg.configurations:\n",
    "    lines_c = [plt.color_sequences[\"tab20\"][i + 1], plt.color_sequences[\"tab20\"][i]]\n",
    "    model_colors_surv_lines[n[model_name_label]] = lines_c\n",
    "    i += 2\n",
    "\n",
    "fig_roc = plt.figure(figsize=fig_roc_size, layout=\"constrained\")\n",
    "ax_roc = fig_roc.add_subplot()\n",
    "\n",
    "test_scores_str = \"\"\n",
    "test_scores_names = [\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"ROC AUC\"]\n",
    "test_scores = {}\n",
    "\n",
    "surv_ylabel = True\n",
    "\n",
    "# TEST\n",
    "input_features = pd.read_csv(\"../data/Braun_2020_ALL_UNIQUE_final_reduced.csv\")\n",
    "input_features[\"TrainTestStatus\"] = [\"Test\"] * input_features.shape[0]\n",
    "# Add one mock Train sample:\n",
    "mock_train_df = pd.read_csv(\"../data/Braun_2020_ALL_UNIQUE_final_reduced.csv\")\n",
    "mock_train_df = mock_train_df[mock_train_df[\"TrainTestStatus\"] == \"Train\"]\n",
    "input_features = pd.concat([mock_train_df, input_features], axis=0, ignore_index=True)\n",
    "# mock_train_row = input_features.iloc[0:1]\n",
    "# mock_train_row[\"TrainTestStatus\"] = [\"Train\"]\n",
    "# input_features = pd.concat([mock_train_row, input_features], axis=0, ignore_index=True)\n",
    "# TEST\n",
    "\n",
    "for config in tqdm(cfg.configurations[3:4], leave=False):\n",
    "    dimension_of_embedding_vectors = config[mut_vec_len_label]\n",
    "    clf_params = config[clf_params_label]\n",
    "    model_name = config[model_name_label]\n",
    "\n",
    "    # Generate input data\n",
    "    _, _, test_data, test_y, test_pfs, _, _ = ds.transforming_Braun_dataset(\n",
    "        config,\n",
    "        dimension_of_embedding_vectors=dimension_of_embedding_vectors,\n",
    "        cut_input_params=cut_input_params,\n",
    "        input_features_file=input_features,\n",
    "    )\n",
    "\n",
    "    # Load SVM model and run test data\n",
    "\n",
    "    clf_file = svm_model_file(model_name, cut_input_params)\n",
    "    with open(clf_file, \"rb\") as f:\n",
    "        clf = pickle.load(f)\n",
    "\n",
    "    svm_linear_preds = clf.predict(test_data)\n",
    "    probs = clf.predict_proba(test_data)\n",
    "    probSV = [i[1] for i in probs]\n",
    "    svm_prob = pd.DataFrame(probSV)\n",
    "\n",
    "    probs = svm_prob.values.flatten()\n",
    "    fpr, tpr, _ = roc_curve(test_y, probs)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    prec = precision_score(test_y, svm_linear_preds)\n",
    "    rec = recall_score(test_y, svm_linear_preds)\n",
    "    f1 = f1_score(test_y, svm_linear_preds)\n",
    "    acc = accuracy_score(test_y, svm_linear_preds)\n",
    "    rauc = roc_auc_score(test_y, probs)\n",
    "\n",
    "    ba = balanced_accuracy_score(test_y, svm_linear_preds)\n",
    "\n",
    "    test_scores_str += f\"{model_name}:\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[0]}: {acc:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[1]}: {prec:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[2]}: {rec:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[3]}: {f1:.3f}\\n\"\n",
    "    test_scores_str += f\"  {test_scores_names[4]}: {rauc:.3f}\\n\"\n",
    "    test_scores_str += f\"    _MEAN_: {(acc + prec + rec + f1 + rauc) / 5:.3f}\\n\"\n",
    "    test_scores_str += f\"    BA: {ba:.3f}\\n\"\n",
    "\n",
    "    test_scores[model_name] = {}\n",
    "    test_scores[model_name][test_scores_names[0]] = acc\n",
    "    test_scores[model_name][test_scores_names[1]] = prec\n",
    "    test_scores[model_name][test_scores_names[2]] = rec\n",
    "    test_scores[model_name][test_scores_names[3]] = f1\n",
    "    test_scores[model_name][test_scores_names[4]] = rauc\n",
    "\n",
    "    _ = ax_roc.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        label=f\"{model_name} (AUC={roc_auc:.3f})\",\n",
    "        color=model_colors[model_name],\n",
    "        linewidth=1,\n",
    "    )\n",
    "\n",
    "    # Response prediction\n",
    "    time_event = test_pfs.reset_index(drop=True)\n",
    "    censoring = pd.Series([1 if p == -1 else 0 for p in svm_linear_preds])\n",
    "    labx = pd.Series([\"Responders\" if y == 1 else \"Non-responders\" for y in test_y])\n",
    "    surv_pred = km.fit(time_event, censoring, labx)\n",
    "    logrank_p = surv_pred[\"logrank_P\"]\n",
    "    fig_surv, ax_surv = km.plot(\n",
    "        surv_pred,\n",
    "        figsize=fig_surv_size,\n",
    "        fontsize=main_font_size,\n",
    "        cmap=model_colors_surv_lines[model_name],\n",
    "    )\n",
    "    # old_title = ax_surv.get_title()\n",
    "    # p_value_str = old_title.replace(\"Survival function\\n\", \"\")\n",
    "    ax_surv.set_title(None)\n",
    "    ax_surv.text(\n",
    "        0.85,\n",
    "        0.1,\n",
    "        f\"Logrank P-value:\\n{logrank_p:.3e}\",\n",
    "        ha=\"right\",\n",
    "        va=\"bottom\",\n",
    "        transform=ax_surv.transAxes,\n",
    "        fontsize=text_font_size,\n",
    "    )\n",
    "    ax_surv.set_title(f\"{model_name}\")\n",
    "    # ax_surv.set_title(\n",
    "    #     old_title.replace(\"Survival function\", f\"{model_name}\"),\n",
    "    #     {\"fontsize\": label_font_size},\n",
    "    # )\n",
    "    ax_surv.tick_params(length=3.5)\n",
    "    ax_surv.set_xlabel(\"Time (months)\")\n",
    "    if surv_ylabel:\n",
    "        ax_surv.set_ylabel(\"Probability of response\")\n",
    "        surv_ylabel = False\n",
    "    ax_surv.legend(\n",
    "        handlelength=0.8,\n",
    "        handletextpad=0.5,\n",
    "        labelspacing=0.8,\n",
    "        fontsize=legend_font_size,\n",
    "        loc=\"upper right\",\n",
    "        # loc=\"upper center\",\n",
    "        # bbox_to_anchor=(0.5, -0.4),\n",
    "    )\n",
    "    for fmt in [\"pdf\", \"png\"]:\n",
    "        fig_surv.savefig(\n",
    "            validation_dir.joinpath(\n",
    "                f\"val-response-prediction-{model_name.replace(' ', '_')}{name_suffix}.{fmt}\"\n",
    "            ),\n",
    "            bbox_inches=\"tight\",\n",
    "            pad_inches=0.03,\n",
    "            dpi=600,\n",
    "        )\n",
    "\n",
    "# ROC\n",
    "ax_roc.set_xlabel(\"False Positive Rate\")\n",
    "ax_roc.set_ylabel(\"True Positive Rate\")\n",
    "# ax_roc.set_title(\"ROC Curves for different levels\", {\"fontsize\": label_font_size})\n",
    "ax_roc.legend(loc=\"lower right\", fontsize=legend_font_size)\n",
    "for fmt in [\"pdf\", \"png\"]:\n",
    "    fig_roc.savefig(\n",
    "        validation_dir.joinpath(f\"val-roc-curves{name_suffix}.{fmt}\"), dpi=600\n",
    "    )\n",
    "\n",
    "# Print and save scores to a file\n",
    "print(test_scores_str)\n",
    "with open(validation_dir.joinpath(f\"val-scores{name_suffix}.txt\"), \"w\") as f:\n",
    "    f.write(test_scores_str)\n",
    "\n",
    "# Scores figure\n",
    "m_names = list(test_scores.keys())\n",
    "s_num = len(test_scores_names)\n",
    "x = np.arange(s_num)\n",
    "b_pos = np.arange(\n",
    "    fig_sc_bar_width / 2 * (1 - m_num),\n",
    "    fig_sc_bar_width / 2 * (m_num - 1),\n",
    "    fig_sc_bar_width,\n",
    ")\n",
    "fig_sc = plt.figure(figsize=fig_sc_size, layout=\"constrained\")\n",
    "ax_sc = fig_sc.add_subplot()\n",
    "for n, p in zip(m_names, b_pos):\n",
    "    _ = ax_sc.bar(\n",
    "        x + p,\n",
    "        list(test_scores[n].values()),\n",
    "        width=fig_sc_bar_width,\n",
    "        label=n,\n",
    "        color=model_colors[n],\n",
    "        zorder=10,\n",
    "    )\n",
    "ax_sc.set_xticks(x, test_scores_names)\n",
    "ax_sc.grid(axis=\"y\")\n",
    "_ = ax_sc.legend(\n",
    "    fontsize=legend_font_size, ncols=m_num, loc=\"lower right\", bbox_to_anchor=(1, 1)\n",
    ")\n",
    "for fmt in [\"pdf\", \"png\"]:\n",
    "    fig_sc.savefig(validation_dir.joinpath(f\"val-scores{name_suffix}.{fmt}\"), dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "svn-opti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
